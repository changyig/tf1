{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3.5\n",
    "# -*- coding: utf-8 -*- \n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 1280\n",
    "WIDTH = 32\n",
    "HEIGHT = 40\n",
    "NUM_CLASSES = 34\n",
    "iterations = 30\n",
    "SAVER_DIR = \"train-saver/digits/\"\n",
    "LETTERS_DIGITS = (\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"J\",\"K\",\"L\",\"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\")\n",
    "license_num = \"\"\n",
    "time_begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入节点，对应于图片像素值矩阵集合和图片标签(即所代表的数字)\n",
    "x = tf.placeholder(tf.float32, shape=[None, SIZE])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES])\n",
    "x_image = tf.reshape(x, [-1, WIDTH, HEIGHT, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 定义卷积函数\n",
    "def conv_layer(inputs, W, b, conv_strides, kernel_size, pool_strides, padding):\n",
    "    L1_conv = tf.nn.conv2d(inputs, W, strides=conv_strides, padding=padding)\n",
    "    L1_relu = tf.nn.relu(L1_conv + b)\n",
    "    return tf.nn.max_pool(L1_relu, ksize=kernel_size, strides=pool_strides, padding='SAME')\n",
    " \n",
    "# 定义全连接层函数\n",
    "def full_connect(inputs, W, b):\n",
    "    return tf.nn.relu(tf.matmul(inputs, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1280)\n",
      "(200, 34)\n",
      "(200, 1280)\n",
      "(4285, 1280)\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__' :\n",
    "    \n",
    "    # 第一次遍历图片目录是为了获取图片总数\n",
    "    input_count = 0\n",
    "    for i in range(0,NUM_CLASSES):\n",
    "        dir = './car-datasets/tf_car_license_dataset/train_images/training-set/%s/' % i      # 这里可以改成你自己的图片目录，i为分类标签\n",
    "        for rt, dirs, files in os.walk(dir):\n",
    "            for filename in files:\n",
    "                input_count += 1\n",
    " \n",
    "    # 定义对应维数和各维长度的数组\n",
    "    input_images = np.array([[0]*SIZE for i in range(input_count)])\n",
    "    input_labels = np.array([[0]*NUM_CLASSES for i in range(input_count)])\n",
    " \n",
    "  # 第二次遍历图片目录是为了生成图片数据和标签\n",
    "    index = 0\n",
    "    for i in range(0,NUM_CLASSES):\n",
    "        dir = './car-datasets/tf_car_license_dataset/train_images/training-set/%s/' % i     # 这里可以改成你自己的图片目录，i为分类标签\n",
    "        for rt, dirs, files in os.walk(dir):\n",
    "            for filename in files:\n",
    "                filename = dir + filename\n",
    "                img = Image.open(filename)\n",
    "                width = img.size[0]\n",
    "                height = img.size[1]\n",
    "                for h in range(0, height):\n",
    "                    for w in range(0, width):\n",
    "                        # 通过这样的处理，使数字的线条变细，有利于提高识别准确率\n",
    "                        if img.getpixel((w, h)) > 230:\n",
    "                            input_images[index][w+h*width] = 0\n",
    "                        else:\n",
    "                            input_images[index][w+h*width] = 1\n",
    "                input_labels[index][i] = 1\n",
    "                index += 1\n",
    " \n",
    "  # 第一次遍历图片目录是为了获取图片总数\n",
    "    val_count = 0\n",
    "    for i in range(0,NUM_CLASSES):\n",
    "        dir = './car-datasets/tf_car_license_dataset/train_images/validation-set/%s/' % i      # 这里可以改成你自己的图片目录，i为分类标签\n",
    "        for rt, dirs, files in os.walk(dir):\n",
    "            for filename in files:\n",
    "                val_count += 1\n",
    " \n",
    "    # 定义对应维数和各维长度的数组\n",
    "    val_images = np.array([[0]*SIZE for i in range(val_count)])\n",
    "    val_labels = np.array([[0]*NUM_CLASSES for i in range(val_count)])\n",
    "    print(val_images.shape)\n",
    "    print(val_labels.shape)\n",
    "    # 第二次遍历图片目录是为了生成图片数据和标签\n",
    "    index = 0\n",
    "    for i in range(0,NUM_CLASSES):\n",
    "        dir = './car-datasets/tf_car_license_dataset/train_images/validation-set/%s/' % i     # 这里可以改成你自己的图片目录，i为分类标签\n",
    "        for rt, dirs, files in os.walk(dir):\n",
    "            for filename in files:\n",
    "                filename = dir + filename\n",
    "                img = Image.open(filename)\n",
    "                width = img.size[0]\n",
    "                height = img.size[1]\n",
    "                for h in range(0, height):\n",
    "                    for w in range(0, width):\n",
    "                    # 通过这样的处理，使数字的线条变细，有利于提高识别准确率\n",
    "                        if img.getpixel((w, h)) > 230:\n",
    "                            val_images[index][w+h*width] = 0\n",
    "                        else:\n",
    "                            val_images[index][w+h*width] = 1\n",
    "                val_labels[index][i] = 1\n",
    "                index += 1\n",
    "    print(val_images.shape)\n",
    "    print(input_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-c35d5e60584f>:38: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "读取图片文件耗费时间：1437秒\n",
      "一共读取了 4285 个训练图像， 4285 个标签\n",
      "训练数据集分成 72 批, 前面每批 60 个数据，最后一批 25 个数据\n",
      "第 0 次训练迭代: 准确率 9.50000%\n",
      "第 5 次训练迭代: 准确率 65.00000%\n",
      "第 10 次训练迭代: 准确率 88.00000%\n",
      "第 15 次训练迭代: 准确率 95.50000%\n",
      "第 20 次训练迭代: 准确率 97.00000%\n",
      "第 25 次训练迭代: 准确率 97.50000%\n",
      "完成训练!\n",
      "训练耗费时间：611秒\n",
      "不存在训练数据保存目录，现在创建保存目录\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 第一个卷积层\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([8, 8, 1, 16], stddev=0.1), name=\"W_conv1\")\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1, shape=[16]), name=\"b_conv1\")\n",
    "    conv_strides = [1, 1, 1, 1]\n",
    "    kernel_size = [1, 2, 2, 1]\n",
    "    pool_strides = [1, 2, 2, 1]\n",
    "    L1_pool = conv_layer(x_image, W_conv1, b_conv1, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "  \n",
    "    # 第二个卷积层\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 16, 32], stddev=0.1), name=\"W_conv2\")\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1, shape=[32]), name=\"b_conv2\")\n",
    "    conv_strides = [1, 1, 1, 1]\n",
    "    kernel_size = [1, 1, 1, 1]\n",
    "    pool_strides = [1, 1, 1, 1]\n",
    "    L2_pool = conv_layer(L1_pool, W_conv2, b_conv2, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "  \n",
    "  \n",
    "    # 全连接层\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([16 * 20 * 32, 512], stddev=0.1), name=\"W_fc1\")\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[512]), name=\"b_fc1\")\n",
    "    h_pool2_flat = tf.reshape(L2_pool, [-1, 16 * 20*32])\n",
    "    h_fc1 = full_connect(h_pool2_flat, W_fc1, b_fc1)\n",
    "  \n",
    "  \n",
    "    # dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "  \n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "  \n",
    "  \n",
    "    # readout层\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([512, NUM_CLASSES], stddev=0.1), name=\"W_fc2\")\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b_fc2\")\n",
    "  \n",
    "    # 定义优化器和训练op\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.AdamOptimizer((1e-4)).minimize(cross_entropy)\n",
    "  \n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    time_elapsed = time.time() - time_begin\n",
    "    print(\"读取图片文件耗费时间：%d秒\" % time_elapsed)\n",
    "    time_begin = time.time()\n",
    "  \n",
    "    print (\"一共读取了 %s 个训练图像， %s 个标签\" % (input_count, input_count))\n",
    "  \n",
    "    # 设置每次训练op的输入个数和迭代次数，这里为了支持任意图片总数，定义了一个余数remainder，譬如，如果每次训练op的输入个数为60，图片总数为150张，则前面两次各输入60张，最后一次输入30张（余数30）\n",
    "    batch_size = 60\n",
    "    iterations = iterations\n",
    "    batches_count = int(input_count / batch_size)\n",
    "    remainder = input_count % batch_size\n",
    "    print (\"训练数据集分成 %s 批, 前面每批 %s 个数据，最后一批 %s 个数据\" % (batches_count+1, batch_size, remainder))\n",
    "  \n",
    "    # 执行训练迭代\n",
    "    for it in range(iterations):\n",
    "      # 这里的关键是要把输入数组转为np.array\n",
    "        for n in range(batches_count):\n",
    "            train_step.run(feed_dict={x: input_images[n*batch_size:(n+1)*batch_size], y_: input_labels[n*batch_size:(n+1)*batch_size], keep_prob: 0.5})\n",
    "        if remainder > 0:\n",
    "            start_index = batches_count * batch_size\n",
    "            train_step.run(feed_dict={x: input_images[start_index:input_count-1], y_: input_labels[start_index:input_count-1], keep_prob: 0.5})\n",
    "  \n",
    "      # 每完成五次迭代，判断准确度是否已达到100%，达到则退出迭代循环\n",
    "        iterate_accuracy = 0\n",
    "        if it%5 == 0:\n",
    "            iterate_accuracy = accuracy.eval(feed_dict={x: val_images, y_: val_labels, keep_prob: 1.0})\n",
    "            print ('第 %d 次训练迭代: 准确率 %0.5f%%' % (it, iterate_accuracy*100))\n",
    "        if iterate_accuracy >= 0.9999 and it >= iterations:\n",
    "            break\n",
    "  \n",
    "    print ('完成训练!')\n",
    "    time_elapsed = time.time() - time_begin\n",
    "    print (\"训练耗费时间：%d秒\" % time_elapsed)\n",
    "    time_begin = time.time()\n",
    "  \n",
    "    # 保存训练结果\n",
    "    if not os.path.exists(SAVER_DIR):\n",
    "        print ('不存在训练数据保存目录，现在创建保存目录')\n",
    "        os.makedirs(SAVER_DIR)\n",
    "    # 初始化saver\n",
    "    saver = tf.train.Saver()      \n",
    "    saver_path = saver.save(sess, \"%smodel.ckpt\"%(SAVER_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train-saver/digits/model.ckpt\n",
      "概率： [N 100.00%]  [Q 0.00%]  [V 0.00%]\n",
      "概率： [C 99.38%]  [L 0.47%]  [U 0.01%]\n",
      "车牌编号是: [NC]\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    saver = tf.train.import_meta_graph(\"%smodel.ckpt.meta\"%(SAVER_DIR))\n",
    "    with tf.Session() as sess:\n",
    "        model_file=tf.train.latest_checkpoint(SAVER_DIR)\n",
    "        saver.restore(sess, model_file)\n",
    "  \n",
    "        # 第一个卷积层\n",
    "        W_conv1 = sess.graph.get_tensor_by_name(\"W_conv1:0\")\n",
    "        b_conv1 = sess.graph.get_tensor_by_name(\"b_conv1:0\")\n",
    "        conv_strides = [1, 1, 1, 1]\n",
    "        kernel_size = [1, 2, 2, 1]\n",
    "        pool_strides = [1, 2, 2, 1]\n",
    "        L1_pool = conv_layer(x_image, W_conv1, b_conv1, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "        # 第二个卷积层\n",
    "        W_conv2 = sess.graph.get_tensor_by_name(\"W_conv2:0\")\n",
    "        b_conv2 = sess.graph.get_tensor_by_name(\"b_conv2:0\")\n",
    "        conv_strides = [1, 1, 1, 1]\n",
    "        kernel_size = [1, 1, 1, 1]\n",
    "        pool_strides = [1, 1, 1, 1]\n",
    "        L2_pool = conv_layer(L1_pool, W_conv2, b_conv2, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "\n",
    "        # 全连接层\n",
    "        W_fc1 = sess.graph.get_tensor_by_name(\"W_fc1:0\")\n",
    "        b_fc1 = sess.graph.get_tensor_by_name(\"b_fc1:0\")\n",
    "        h_pool2_flat = tf.reshape(L2_pool, [-1, 16 * 20*32])\n",
    "        h_fc1 = full_connect(h_pool2_flat, W_fc1, b_fc1)\n",
    "\n",
    "\n",
    "        # dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "  \n",
    "  \n",
    "        # readout层\n",
    "        W_fc2 = sess.graph.get_tensor_by_name(\"W_fc2:0\")\n",
    "        b_fc2 = sess.graph.get_tensor_by_name(\"b_fc2:0\")\n",
    "\n",
    "        # 定义优化器和训练op\n",
    "        conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "  \n",
    "        for n in range(1,3):\n",
    "            path = \"test_images/car/%s.bmp\" % (n)\n",
    "            img = Image.open(path)\n",
    "            width = img.size[0]\n",
    "            height = img.size[1]\n",
    "\n",
    "            img_data = [[0]*SIZE for i in range(1)]\n",
    "            for h in range(0, height):\n",
    "                for w in range(0, width):\n",
    "                    if img.getpixel((w, h)) < 190:\n",
    "                        img_data[0][w+h*width] = 1\n",
    "                    else:\n",
    "                        img_data[0][w+h*width] = 0\n",
    "       \n",
    "            result = sess.run(conv, feed_dict = {x: np.array(img_data), keep_prob: 1.0})\n",
    "       \n",
    "            max1 = 0\n",
    "            max2 = 0\n",
    "            max3 = 0\n",
    "            max1_index = 0\n",
    "            max2_index = 0\n",
    "            max3_index = 0\n",
    "            for j in range(NUM_CLASSES):\n",
    "                if result[0][j] > max1:\n",
    "                    max1 = result[0][j]\n",
    "                    max1_index = j\n",
    "                    continue\n",
    "                if (result[0][j]>max2) and (result[0][j]<=max1):\n",
    "                    max2 = result[0][j]\n",
    "                    max2_index = j\n",
    "                    continue\n",
    "                if (result[0][j]>max3) and (result[0][j]<=max2):\n",
    "                    max3 = result[0][j]\n",
    "                    max3_index = j\n",
    "                    continue\n",
    "       \n",
    "            license_num = license_num + LETTERS_DIGITS[max1_index]\n",
    "            print (\"概率： [%s %0.2f%%]  [%s %0.2f%%]  [%s %0.2f%%]\" % (LETTERS_DIGITS[max1_index],max1*100, LETTERS_DIGITS[max2_index],max2*100, LETTERS_DIGITS[max3_index],max3*100))\n",
    "       \n",
    "        print (\"车牌编号是: [%s]\" % license_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
